{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1b781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "dataset_dir = '../datasets/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcd6fc",
   "metadata": {},
   "source": [
    "# Downloading and extracting captions\n",
    "\n",
    "For MS Coco TensorFlow Datasets is employed.  \n",
    "For Open Images V7 the json and csv files were downloaded by start.sh.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2c382",
   "metadata": {},
   "source": [
    "## MS Coco\n",
    "\n",
    "First the dataset will be downloaded from TensorFlow.  \n",
    "After the dataset is downloaded a first filter is applied, all images with less than 5 individuals are filtered out.  \n",
    "The filtered images are then stored in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder ../datasets/coco/coco_captions/2014/1.1.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ../datasets/coco/coco_captions/2014/1.1.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 223.71 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 161.48 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 132.11 url/s]\n",
      "Dl Completed...:  50%|█████     | 1/2 [00:00<00:00, 94.52 url/s] \n",
      "Dl Completed...:  33%|███▎      | 1/3 [00:00<00:00, 73.76 url/s]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 131.54 url/s]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 115.15 url/s]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 105.26 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 92.41 url/s] \n",
      "Dl Completed...:  40%|████      | 2/5 [00:00<00:00, 82.26 url/s]\n",
      "Dl Completed...:  60%|██████    | 3/5 [00:00<00:00, 116.59 url/s]\n",
      "Dl Completed...:  60%|██████    | 3/5 [00:00<00:00, 110.50 url/s]\n",
      "Dl Completed...:  60%|██████    | 3/5 [00:00<00:00, 102.40 url/s]\n",
      "Dl Completed...:  50%|█████     | 3/6 [00:00<00:00, 94.41 url/s] \n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:00<00:00, 87.23 url/s]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:01<00:01,  2.08 url/s]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:01<00:01,  2.02 url/s]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:01<00:02,  2.00 url/s]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:01<00:02,  1.71 url/s]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:05<00:07,  1.84s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:05<00:07,  1.93s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:05<00:07,  1.95s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:06<00:08,  2.05s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:07<00:10,  2.64s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:08<00:10,  2.74s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:08<00:11,  2.85s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:08<00:11,  2.86s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:09<00:13,  3.26s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:10<00:13,  3.34s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:10<00:13,  3.36s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:10<00:13,  3.42s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:10<00:14,  3.54s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:10<00:14,  3.63s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:11<00:15,  3.76s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:11<00:15,  3.89s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:11<00:15,  3.92s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:12<00:16,  4.02s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:12<00:16,  4.05s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:12<00:16,  4.07s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:12<00:16,  4.14s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:12<00:17,  4.32s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:13<00:17,  4.34s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:13<00:17,  4.39s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:13<00:18,  4.51s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:13<00:18,  4.53s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:13<00:18,  4.58s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:13<00:18,  4.64s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:14<00:19,  4.78s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:14<00:19,  4.79s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:14<00:19,  4.82s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:14<00:19,  4.84s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:14<00:19,  4.91s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:14<00:19,  4.96s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:15<00:20,  5.14s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:15<00:20,  5.15s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:15<00:20,  5.17s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:15<00:20,  5.21s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:15<00:21,  5.28s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:21,  5.35s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:22,  5.50s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:22,  5.51s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:22,  5.53s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:22,  5.53s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:22,  5.59s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:16<00:22,  5.65s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:17<00:22,  5.75s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:17<00:23,  5.94s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:17<00:23,  5.98s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:18<00:24,  6.00s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:18<00:24,  6.21s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:18<00:25,  6.26s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:18<00:25,  6.27s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:19<00:25,  6.36s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:19<00:25,  6.46s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:19<00:26,  6.56s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:20<00:26,  6.67s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:20<00:26,  6.72s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:20<00:27,  6.94s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:20<00:27,  6.97s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:21<00:28,  7.01s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:21<00:28,  7.10s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:21<00:28,  7.19s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:22<00:29,  7.34s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:22<00:29,  7.45s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:22<00:30,  7.51s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:22<00:30,  7.57s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:23<00:30,  7.70s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:23<00:31,  7.75s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:23<00:31,  7.82s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:23<00:31,  7.95s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:24<00:32,  8.02s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:24<00:32,  8.19s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:24<00:32,  8.20s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:24<00:32,  8.22s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:25<00:33,  8.39s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:25<00:33,  8.45s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:25<00:34,  8.61s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:26<00:34,  8.71s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:26<00:35,  8.77s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:26<00:35,  8.83s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:27<00:36,  9.00s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:27<00:36,  9.18s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:27<00:36,  9.19s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:28<00:37,  9.34s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:28<00:37,  9.38s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:28<00:38,  9.50s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:28<00:38,  9.66s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:29<00:38,  9.68s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:29<00:38,  9.74s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:30<00:40, 10.02s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:30<00:40, 10.02s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:30<00:40, 10.11s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:30<00:40, 10.12s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:31<00:41, 10.36s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:31<00:41, 10.45s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:31<00:42, 10.54s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:32<00:42, 10.75s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:32<00:43, 10.76s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:32<00:43, 10.94s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:33<00:44, 11.04s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:33<00:44, 11.20s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:33<00:44, 11.22s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:34<00:45, 11.41s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:34<00:46, 11.57s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:34<00:46, 11.62s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:35<00:47, 11.78s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:35<00:47, 11.82s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:36<00:48, 12.02s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:36<00:48, 12.24s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:36<00:48, 12.24s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:37<00:49, 12.40s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:37<00:49, 12.43s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:38<00:50, 12.73s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:38<00:51, 12.90s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:38<00:51, 12.93s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:39<00:52, 13.06s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:39<00:53, 13.25s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:40<00:53, 13.38s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:40<00:53, 13.45s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:40<00:54, 13.54s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:41<00:54, 13.70s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:41<00:55, 13.90s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:41<00:55, 13.95s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:42<00:56, 14.20s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:42<00:57, 14.26s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:43<00:57, 14.43s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:43<00:58, 14.51s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:43<00:58, 14.54s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:44<00:59, 14.85s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:45<01:00, 15.10s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:45<01:00, 15.12s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:45<01:00, 15.14s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:46<01:02, 15.52s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:47<01:02, 15.68s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:47<01:02, 15.68s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:47<01:02, 15.69s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:48<01:04, 16.15s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:48<01:04, 16.20s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:49<01:05, 16.36s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:49<01:05, 16.44s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:50<01:07, 16.82s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:50<01:07, 16.89s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:51<01:08, 17.02s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:51<01:08, 17.13s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:52<01:09, 17.47s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:52<01:10, 17.54s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:52<01:10, 17.61s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:53<01:10, 17.72s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:53<01:11, 17.94s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:53<01:11, 17.95s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:54<01:12, 18.03s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:54<01:12, 18.11s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:54<01:13, 18.26s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:54<01:13, 18.32s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:55<01:13, 18.35s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:55<01:14, 18.57s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:55<01:14, 18.58s/ url]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:55<01:14, 18.64s/ url]\n",
      "Extraction completed...: 0 file [00:55, ? file/s]\n",
      "Dl Size...: 100%|█████████▉| 255523871/255561649 [00:55<00:00, 4564439.92 MiB/s]\n",
      "Dl Completed...:  43%|████▎     | 3/7 [00:55<01:14, 18.66s/ url]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m person_id = \u001b[32m0\u001b[39m \u001b[38;5;66;03m# this values is found in the official MS Coco documentation\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset, folder \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28mset\u001b[39m = \u001b[43mtfds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# filter out all images that do not contain at least one person\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m part_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/load.py:666\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs, file_format)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[32m    542\u001b[39m \n\u001b[32m    543\u001b[39m \u001b[33;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    658\u001b[39m \u001b[33;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[32m    660\u001b[39m dbuilder = _fetch_builder(\n\u001b[32m    661\u001b[39m     name=name,\n\u001b[32m    662\u001b[39m     data_dir=data_dir,\n\u001b[32m    663\u001b[39m     builder_kwargs=builder_kwargs,\n\u001b[32m    664\u001b[39m     try_gcs=try_gcs,\n\u001b[32m    665\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    669\u001b[39m   as_dataset_kwargs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/load.py:518\u001b[39m, in \u001b[36m_download_and_prepare_builder\u001b[39m\u001b[34m(dbuilder, download, download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m    517\u001b[39m   download_and_prepare_kwargs = download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m   \u001b[43mdbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:763\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, download_dir, download_config, file_format, permissions)\u001b[39m\n\u001b[32m    761\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.read_from_directory(\u001b[38;5;28mself\u001b[39m.data_dir)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[32m    769\u001b[39m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[32m    770\u001b[39m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[32m    771\u001b[39m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[32m    772\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.download_size = dl_manager.downloaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1808\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_config.max_examples_per_split == \u001b[32m0\u001b[39m:\n\u001b[32m   1806\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m split_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[32m   1811\u001b[39m split_dict = splits_lib.SplitDict(split_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1758\u001b[39m, in \u001b[36mGeneratorBasedBuilder._generate_splits\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1757\u001b[39m   optional_pipeline_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1758\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[32m   1764\u001b[39m split_generators = split_builder.normalize_legacy_split_generators(\n\u001b[32m   1765\u001b[39m     split_generators=split_generators,\n\u001b[32m   1766\u001b[39m     generator_fn=\u001b[38;5;28mself\u001b[39m._generate_examples,\n\u001b[32m   1767\u001b[39m     is_beam=\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[32m   1768\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/object_detection/coco_captions.py:86\u001b[39m, in \u001b[36mCocoCaptions._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Only keep the coco train and validation sets.\u001b[39;00m\n\u001b[32m     84\u001b[39m coco_splits = {\n\u001b[32m     85\u001b[39m     split.name: split\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCocoCaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m }\n\u001b[32m     88\u001b[39m coco_train_split = coco_splits[tfds.Split.TRAIN]\n\u001b[32m     89\u001b[39m coco_val_split = coco_splits[tfds.Split.VALIDATION]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/object_detection/coco.py:245\u001b[39m, in \u001b[36mCoco._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# DownloadManager memoize the url, so duplicate urls will only be downloaded\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# once.\u001b[39;00m\n\u001b[32m    244\u001b[39m root_url = \u001b[33m'\u001b[39m\u001b[33mhttp://images.cocodataset.org/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m extracted_paths = \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_url\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m splits = []\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builder_config.splits:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:754\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._downloader.tqdm():\n\u001b[32m    753\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extractor.tqdm():\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise\u001b[39m\u001b[34m(map_fn, all_inputs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tree/__init__.py:428\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structures, **kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[32m1\u001b[39m:]:\n\u001b[32m    426\u001b[39m   assert_same_structure(structures[\u001b[32m0\u001b[39m], other, check_types=check_types)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[32m0\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise.<locals>.<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = tree.map_structure(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises)  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/promise/promise.py:511\u001b[39m, in \u001b[36mPromise.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# type: (Optional[float]) -> T\u001b[39;00m\n\u001b[32m    510\u001b[39m     target = \u001b[38;5;28mself\u001b[39m._target()\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEFAULT_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._target_settled_value(_raise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/promise/promise.py:506\u001b[39m, in \u001b[36mPromise._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m# type: (Optional[float]) -> None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/promise/promise.py:502\u001b[39m, in \u001b[36mPromise.wait\u001b[39m\u001b[34m(cls, promise, timeout)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(\u001b[38;5;28mcls\u001b[39m, promise, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# type: (Promise, Optional[float]) -> None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     \u001b[43masync_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/promise/async_.py:117\u001b[39m, in \u001b[36mAsync.wait\u001b[39m\u001b[34m(self, promise, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m promise.is_pending:\n\u001b[32m    114\u001b[39m         \u001b[38;5;66;03m# We return if the promise is already\u001b[39;00m\n\u001b[32m    115\u001b[39m         \u001b[38;5;66;03m# fulfilled or rejected\u001b[39;00m\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master/master-thesis/.venv/lib/python3.12/site-packages/promise/schedulers/immediate.py:25\u001b[39m, in \u001b[36mImmediateScheduler.wait\u001b[39m\u001b[34m(self, promise, timeout)\u001b[39m\n\u001b[32m     22\u001b[39m     e.set()\n\u001b[32m     24\u001b[39m promise._then(on_resolve_or_reject, on_resolve_or_reject)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m waited = \u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m waited:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "decoders = {\n",
    "    'image': tfds.decode.SkipDecoding(),\n",
    "    'image/filename': tfds.decode.SkipDecoding(),\n",
    "}\n",
    "\n",
    "# DataFrame that stores all filtered prompts\n",
    "coco_df = pd.DataFrame(columns=['image_id', 'caption', 'dataset'])\n",
    "\n",
    "# coco includes the images and coco_captions stores the captions that are used as prompts\n",
    "datasets = [('coco_captions', 'coco')]\n",
    "person_id = 0 # this values is found in the official MS Coco documentation\n",
    "for dataset, folder in datasets:\n",
    "    set = tfds.load(dataset, data_dir=os.path.join(dataset_dir, folder), download=True, decoders=decoders)\n",
    "\n",
    "    # filter out all images that do not contain at least one person\n",
    "    for part_set in set:\n",
    "        for element in set[part_set]:\n",
    "            s = sum(map(lambda x: 1 if x == person_id else 0, element['objects']['label']))            \n",
    "            if s >= 5:\n",
    "                for caption in element['captions']['text']:\n",
    "                    if element['image/id'].numpy() == None or len(caption.numpy().decode(\"utf-8\")) == 0:\n",
    "                        continue\n",
    "                    coco_df.loc[len(coco_df)] = [element['image/id'].numpy(), ''.join(caption.numpy().decode(\"utf-8\").splitlines()), 'coco']\n",
    "    coco_df.to_csv(os.path.join(dataset_dir, os.path.join('coco/result_csv', 'person_captions.csv')), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dcfa74",
   "metadata": {},
   "source": [
    "## Open Image V7\n",
    "\n",
    "First all images that contain people are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3404d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_person(text: str) -> bool:\n",
    "    return re.search(r'\\b%s\\b' % (re.escape('person')), text.lower()) is not None \n",
    "\n",
    "\n",
    "def get_imgid_person_from_csv(csv: str, person_labels: list[str]):\n",
    "    csv_df = pd.read_csv(csv)\n",
    "    csv_df = csv_df[csv_df['LabelName'].isin(person_labels)]\n",
    "    csv_df = csv_df['ImageID']\n",
    "    return csv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cc6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting image ids from: open-images/csv/oidv7-test-annotations-human-imagelabels.csv\n",
      "extracting image ids from: open-images/csv/oidv7-test-annotations-machine-imagelabels.csv\n",
      "extracting image ids from: open-images/csv/oidv7-train-annotations-human-imagelabels.csv\n",
      "extracting image ids from: open-images/csv/oidv7-train-annotations-machine-imagelabels.csv\n",
      "extracting image ids from: open-images/csv/oidv7-val-annotations-human-imagelabels.csv\n",
      "extracting image ids from: open-images/csv/oidv7-val-annotations-machine-imagelabels.csv\n"
     ]
    }
   ],
   "source": [
    "# this DataFrame contains all ImageIDs for image containing at least one person.\n",
    "people_ids = pd.DataFrame(columns=['ImageID'])\n",
    "\n",
    "class_desc_dir = os.path.join(dataset_dir, 'open-images/csv/oidv7-class-descriptions.csv')\n",
    "\n",
    "# csv that containt the labels\n",
    "label_csv = ['open-images/csv/oidv7-test-annotations-human-imagelabels.csv',\n",
    "                'open-images/csv/oidv7-test-annotations-machine-imagelabels.csv',\n",
    "                'open-images/csv/oidv7-train-annotations-human-imagelabels.csv',\n",
    "                'open-images/csv/oidv7-train-annotations-machine-imagelabels.csv',\n",
    "                'open-images/csv/oidv7-val-annotations-human-imagelabels.csv',\n",
    "                'open-images/csv/oidv7-val-annotations-machine-imagelabels.csv' ]\n",
    "\n",
    "\n",
    "# getting label name for classes that are people\n",
    "class_df = pd.read_csv(class_desc_dir)\n",
    "\n",
    "person_labels = []\n",
    "\n",
    "#checking if label describes a person\n",
    "for index, rw in class_df.iterrows():\n",
    "    if contains_person(rw['DisplayName']):\n",
    "        person_labels.append(rw['LabelName'])\n",
    "\n",
    "# go through label csv and extract image ids from images containing people\n",
    "people_ids = pd.DataFrame(columns=['ImageID'])\n",
    "for csv in label_csv:\n",
    "    print(f\"extracting image ids from: {csv}\")\n",
    "    people_ids = pd.concat([get_imgid_person_from_csv(os.path.join(dataset_dir, csv), person_labels), people_ids])\n",
    "\n",
    "# make sure that ids are unique\n",
    "people_ids = people_ids.drop_duplicates(subset=['ImageID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6c22",
   "metadata": {},
   "source": [
    "Now all images that don't include at least one person are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbe97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json(path: str, people_ids: pd.DataFrame):\n",
    "    json_df = pd.read_json(path_or_buf=path, lines=True)\n",
    "    \n",
    "    # filter for images with people in it\n",
    "    json_df = json_df[json_df['image_id'].isin(people_ids['ImageID'])]\n",
    "\n",
    "    # only take caption and image id\n",
    "    json_df = json_df[['image_id', 'caption']]\n",
    "    json_df['dataset'] = 'open_images'\n",
    "    return json_df\n",
    "\n",
    "\n",
    "def clean_json_large(path: str, people_ids: pd.DataFrame):\n",
    "    fs = open(path)\n",
    "    captions = pd.DataFrame(columns=['image_id', 'caption', 'dataset'])\n",
    "\n",
    "    for line in fs:\n",
    "        js = json.loads(line)\n",
    "        if js['image_id'] in people_ids['ImageID']:\n",
    "            captions.loc[len(captions)] = [js['image_id'], js['caption'], 'open_images']\n",
    "            \n",
    "    return captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be7f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file <module 'json' from '/usr/lib/python3.12/json/__init__.py'>\n",
      "processing large file: open_images_test_localized_narratives.jsonl, 3533330499 Bytes\n",
      "reading file <module 'json' from '/usr/lib/python3.12/json/__init__.py'>\n",
      "reading file <module 'json' from '/usr/lib/python3.12/json/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "json_dir = os.path.join(dataset_dir, 'open-images/json')\n",
    "\n",
    "captions = pd.DataFrame(columns=['image_id', 'caption', 'dataset'])\n",
    "\n",
    "j = 0\n",
    "GiB =  2 ** 30\n",
    "\n",
    "skip = True\n",
    "\n",
    "for json_file in os.listdir(json_dir):\n",
    "\n",
    "    if os.path.exists(os.path.join(dataset_dir, os.path.join('open-images/result_csv', f'all_people_captions_{j}.csv'))):\n",
    "        j += 1\n",
    "        continue\n",
    "    print(f'reading file {json}')\n",
    "    # check if file exceds two GiB\n",
    "    if os.path.getsize(os.path.join(json_dir, json_file)) > 2 * GiB:\n",
    "        print(f'processing large file: {json_file}, {os.path.getsize(os.path.join(json_dir, json_file))} Bytes')\n",
    "        captions = clean_json_large(os.path.join(json_dir, json_file), people_ids)\n",
    "    else:\n",
    "        captions = clean_json(os.path.join(json_dir, json_file), people_ids)\n",
    "\n",
    "    # make sure that ids are unique\n",
    "    captions = captions.drop_duplicates(subset=['image_id'])\n",
    "\n",
    "    #write to file \n",
    "    captions.to_csv(os.path.join(dataset_dir, os.path.join('open-images/result_csv', f'all_people_captions_{j}.csv')), index=False)\n",
    "    \n",
    "    captions = pd.DataFrame(columns=['image_id', 'caption'])\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0daa4c",
   "metadata": {},
   "source": [
    "## Filter extracted captions\n",
    "First it is checked that a plural word, describing groups of people is inluded.   \n",
    "Afterwards gendered terms are replaced with 'people'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c48fcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_plural_words = [\n",
    "    # Healthcare\n",
    "    \"doctors\", \"surgeons\", \"nurses\", \"therapists\", \"paramedics\", \"dentists\",\n",
    "    \"optometrists\", \"pharmacists\", \"pediatricians\", \"psychiatrists\", \"radiologists\",\n",
    "    \"anesthesiologists\", \"pathologists\", \"endocrinologists\", \"oncologists\", \"cardiologists\",\n",
    "    \"neurologists\", \"dermatologists\", \"general practitioners\", \"midwives\", \"physicians\",\n",
    "    \"emergency responders\", \"clinicians\", \"geneticists\", \"urologists\", \"gastroenterologists\",\n",
    "    \"nephrologists\", \"ophthalmologists\", \"immunologists\", \"allergists\",\n",
    "    \n",
    "    # Education\n",
    "    \"teachers\", \"professors\", \"lecturers\", \"instructors\", \"educators\", \"mentors\",\n",
    "    \"counselors\", \"principals\", \"deans\", \"tutors\", \"coaches\", \"trainers\", \"academics\",\n",
    "    \"scholars\", \"researchers\", \"administrators\",\n",
    "    \n",
    "    # Science, Engineering, and Technology\n",
    "    \"engineers\", \"scientists\", \"programmers\", \"developers\", \"designers\", \"analysts\",\n",
    "    \"mathematicians\", \"statisticians\", \"physicists\", \"chemists\", \"biologists\", \"geologists\",\n",
    "    \"astronomers\", \"economists\", \"sociologists\", \"anthropologists\", \"psychologists\",\n",
    "    \"philosophers\", \"historians\", \"librarians\", \"geographers\", \"computer scientists\",\n",
    "    \"IT specialists\", \"cybersecurity experts\", \"data scientists\", \"system administrators\",\n",
    "    \n",
    "    # Business and Finance\n",
    "    \"accountants\", \"managers\", \"executives\", \"directors\", \"entrepreneurs\", \"investors\",\n",
    "    \"bankers\", \"brokers\", \"salespeople\", \"marketers\", \"consultants\", \"advisors\",\n",
    "    \"strategists\", \"negotiators\", \"coordinators\", \"planners\", \"auditors\", \"traders\",\n",
    "    \"risk managers\",\n",
    "    \n",
    "    # Law, Government, and Public Service\n",
    "    \"lawyers\", \"judges\", \"attorneys\", \"policemen\", \"policewomen\", \"firefighters\",\n",
    "    \"soldiers\", \"marines\", \"officers\", \"agents\", \"diplomats\", \"ambassadors\", \"envoys\",\n",
    "    \"mediators\", \"arbitrators\", \"councillors\", \"mayors\", \"representatives\", \"senators\",\n",
    "    \"congressmen\", \"assemblymen\", \"ministers\", \"bureaucrats\", \"public servants\",\n",
    "    \"legislators\", \"politicians\", \"deputies\", \"commissioners\",\n",
    "    \n",
    "    # Arts and Entertainment\n",
    "    \"musicians\", \"singers\", \"dancers\", \"actors\", \"actresses\", \"painters\", \"sculptors\",\n",
    "    \"writers\", \"poets\", \"novelists\", \"journalists\", \"broadcasters\", \"reporters\",\n",
    "    \"photographers\", \"videographers\", \"models\", \"stylists\", \"illustrators\", \"animators\",\n",
    "    \"composers\", \"orchestrators\", \"producers\", \"performers\", \"entertainers\", \"storytellers\",\n",
    "    \"comedians\", \"magicians\", \"acrobats\", \"improvisers\", \"mimes\", \"puppeteers\",\n",
    "    \n",
    "    # Trades and Manual Labor\n",
    "    \"carpenters\", \"electricians\", \"plumbers\", \"mechanics\", \"drivers\", \"pilots\", \"sailors\",\n",
    "    \"chefs\", \"bakers\", \"butchers\", \"tailors\", \"seamstresses\", \"dressmakers\", \"gardeners\",\n",
    "    \"farmers\", \"fishers\", \"hunters\", \"blacksmiths\", \"welders\", \"masons\", \"bricklayers\",\n",
    "    \"roofers\", \"installers\", \"packers\", \"movers\", \"cleaners\", \"laborers\", \"technicians\",\n",
    "    \"operators\", \"cashiers\", \"clerks\", \"servicemen\", \"waiters\", \"waitresses\", \"bartenders\",\n",
    "    \"baristas\", \"cheerleaders\",\n",
    "    \n",
    "    # Sports and Recreation\n",
    "    \"athletes\", \"runners\", \"swimmers\", \"cyclists\", \"skaters\", \"golfers\", \"tennis players\",\n",
    "    \"footballers\", \"basketball players\", \"baseball players\", \"hockey players\", \"boxers\",\n",
    "    \"wrestlers\", \"martial artists\", \"soccer players\", \"cricketers\", \"fencers\", \"surfers\",\n",
    "    \"skiers\", \"snowboarders\", \"rowers\", \"kayakers\", \"climbers\", \"fishermen\",\n",
    "    \n",
    "    # Miscellaneous and Other Groups\n",
    "    \"volunteers\", \"activists\", \"campaigners\", \"participants\", \"contributors\",\n",
    "    \"collaborators\", \"organizers\", \"innovators\", \"thinkers\", \"visionaries\", \"pioneers\",\n",
    "    \"trendsetters\", \"enthusiasts\", \"collectors\", \"critics\", \"reviewers\", \"bloggers\",\n",
    "    \"vloggers\", \"podcasters\", \"gamers\", \"streamers\", \"followers\", \"subscribers\",\n",
    "    \"examiners\", \"observers\", \"spectators\", \"bystanders\", \"residents\", \"citizens\",\n",
    "    \"inhabitants\", \"locals\", \"immigrants\", \"refugees\", \"delegates\", \"evangelists\",\n",
    "    \"architects\", \"adventurers\", \"explorers\", \"pilgrims\", \"campers\", \"travelers\",\n",
    "    \"tourists\", \"visitors\", \"commuters\", \"shoppers\", \"vendors\", \"merchants\",\n",
    "    \"distributors\", \"consumers\", \"farmhands\", \"contractors\", \"freelancers\", \"interns\",\n",
    "    \"apprentices\", \"trainees\", \"seniors\", \"elders\", \"youths\", \"teens\", \"children\",\n",
    "    \"infants\", \"adults\", \"men\", \"women\", \"couples\", \"siblings\", \"parents\", \"grandparents\",\n",
    "    \"cousins\", \"relatives\", \"classmates\", \"roommates\", \"colleagues\", \"associates\",\n",
    "    \"partners\", \"comrades\", \"confederates\", \"allies\", \"supporters\", \"backers\", \"fans\",\n",
    "    \"members\", \"cadets\", \"rookies\", \"veterans\", \"champions\", \"contenders\", \"finalists\",\n",
    "    \"competitors\", \"medalists\", \"record-holders\", \"brainiacs\", \"geeks\", \"nerds\",\n",
    "    \"whizzes\", \"techies\", \"creatives\", \"ideators\", \"facilitators\", \"moderators\",\n",
    "    \"supervisors\", \"evaluators\", \"assessors\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2689e13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering for word 0/314\n",
      "filtering for word 1/314\n",
      "filtering for word 2/314\n",
      "filtering for word 3/314\n",
      "filtering for word 4/314\n",
      "filtering for word 5/314\n",
      "filtering for word 6/314\n",
      "filtering for word 7/314\n",
      "filtering for word 8/314\n",
      "filtering for word 9/314\n",
      "filtering for word 10/314\n",
      "filtering for word 11/314\n",
      "filtering for word 12/314\n",
      "filtering for word 13/314\n",
      "filtering for word 14/314\n",
      "filtering for word 15/314\n",
      "filtering for word 16/314\n",
      "filtering for word 17/314\n",
      "filtering for word 18/314\n",
      "filtering for word 19/314\n",
      "filtering for word 20/314\n",
      "filtering for word 21/314\n",
      "filtering for word 22/314\n",
      "filtering for word 23/314\n",
      "filtering for word 24/314\n",
      "filtering for word 25/314\n",
      "filtering for word 26/314\n",
      "filtering for word 27/314\n",
      "filtering for word 28/314\n",
      "filtering for word 29/314\n",
      "filtering for word 30/314\n",
      "filtering for word 31/314\n",
      "filtering for word 32/314\n",
      "filtering for word 33/314\n",
      "filtering for word 34/314\n",
      "filtering for word 35/314\n",
      "filtering for word 36/314\n",
      "filtering for word 37/314\n",
      "filtering for word 38/314\n",
      "filtering for word 39/314\n",
      "filtering for word 40/314\n",
      "filtering for word 41/314\n",
      "filtering for word 42/314\n",
      "filtering for word 43/314\n",
      "filtering for word 44/314\n",
      "filtering for word 45/314\n",
      "filtering for word 46/314\n",
      "filtering for word 47/314\n",
      "filtering for word 48/314\n",
      "filtering for word 49/314\n",
      "filtering for word 50/314\n",
      "filtering for word 51/314\n",
      "filtering for word 52/314\n",
      "filtering for word 53/314\n",
      "filtering for word 54/314\n",
      "filtering for word 55/314\n",
      "filtering for word 56/314\n",
      "filtering for word 57/314\n",
      "filtering for word 58/314\n",
      "filtering for word 59/314\n",
      "filtering for word 60/314\n",
      "filtering for word 61/314\n",
      "filtering for word 62/314\n",
      "filtering for word 63/314\n",
      "filtering for word 64/314\n",
      "filtering for word 65/314\n",
      "filtering for word 66/314\n",
      "filtering for word 67/314\n",
      "filtering for word 68/314\n",
      "filtering for word 69/314\n",
      "filtering for word 70/314\n",
      "filtering for word 71/314\n",
      "filtering for word 72/314\n",
      "filtering for word 73/314\n",
      "filtering for word 74/314\n",
      "filtering for word 75/314\n",
      "filtering for word 76/314\n",
      "filtering for word 77/314\n",
      "filtering for word 78/314\n",
      "filtering for word 79/314\n",
      "filtering for word 80/314\n",
      "filtering for word 81/314\n",
      "filtering for word 82/314\n",
      "filtering for word 83/314\n",
      "filtering for word 84/314\n",
      "filtering for word 85/314\n",
      "filtering for word 86/314\n",
      "filtering for word 87/314\n",
      "filtering for word 88/314\n",
      "filtering for word 89/314\n",
      "filtering for word 90/314\n",
      "filtering for word 91/314\n",
      "filtering for word 92/314\n",
      "filtering for word 93/314\n",
      "filtering for word 94/314\n",
      "filtering for word 95/314\n",
      "filtering for word 96/314\n",
      "filtering for word 97/314\n",
      "filtering for word 98/314\n",
      "filtering for word 99/314\n",
      "filtering for word 100/314\n",
      "filtering for word 101/314\n",
      "filtering for word 102/314\n",
      "filtering for word 103/314\n",
      "filtering for word 104/314\n",
      "filtering for word 105/314\n",
      "filtering for word 106/314\n",
      "filtering for word 107/314\n",
      "filtering for word 108/314\n",
      "filtering for word 109/314\n",
      "filtering for word 110/314\n",
      "filtering for word 111/314\n",
      "filtering for word 112/314\n",
      "filtering for word 113/314\n",
      "filtering for word 114/314\n",
      "filtering for word 115/314\n",
      "filtering for word 116/314\n",
      "filtering for word 117/314\n",
      "filtering for word 118/314\n",
      "filtering for word 119/314\n",
      "filtering for word 120/314\n",
      "filtering for word 121/314\n",
      "filtering for word 122/314\n",
      "filtering for word 123/314\n",
      "filtering for word 124/314\n",
      "filtering for word 125/314\n",
      "filtering for word 126/314\n",
      "filtering for word 127/314\n",
      "filtering for word 128/314\n",
      "filtering for word 129/314\n",
      "filtering for word 130/314\n",
      "filtering for word 131/314\n",
      "filtering for word 132/314\n",
      "filtering for word 133/314\n",
      "filtering for word 134/314\n",
      "filtering for word 135/314\n",
      "filtering for word 136/314\n",
      "filtering for word 137/314\n",
      "filtering for word 138/314\n",
      "filtering for word 139/314\n",
      "filtering for word 140/314\n",
      "filtering for word 141/314\n",
      "filtering for word 142/314\n",
      "filtering for word 143/314\n",
      "filtering for word 144/314\n",
      "filtering for word 145/314\n",
      "filtering for word 146/314\n",
      "filtering for word 147/314\n",
      "filtering for word 148/314\n",
      "filtering for word 149/314\n",
      "filtering for word 150/314\n",
      "filtering for word 151/314\n",
      "filtering for word 152/314\n",
      "filtering for word 153/314\n",
      "filtering for word 154/314\n",
      "filtering for word 155/314\n",
      "filtering for word 156/314\n",
      "filtering for word 157/314\n",
      "filtering for word 158/314\n",
      "filtering for word 159/314\n",
      "filtering for word 160/314\n",
      "filtering for word 161/314\n",
      "filtering for word 162/314\n",
      "filtering for word 163/314\n",
      "filtering for word 164/314\n",
      "filtering for word 165/314\n",
      "filtering for word 166/314\n",
      "filtering for word 167/314\n",
      "filtering for word 168/314\n",
      "filtering for word 169/314\n",
      "filtering for word 170/314\n",
      "filtering for word 171/314\n",
      "filtering for word 172/314\n",
      "filtering for word 173/314\n",
      "filtering for word 174/314\n",
      "filtering for word 175/314\n",
      "filtering for word 176/314\n",
      "filtering for word 177/314\n",
      "filtering for word 178/314\n",
      "filtering for word 179/314\n",
      "filtering for word 180/314\n",
      "filtering for word 181/314\n",
      "filtering for word 182/314\n",
      "filtering for word 183/314\n",
      "filtering for word 184/314\n",
      "filtering for word 185/314\n",
      "filtering for word 186/314\n",
      "filtering for word 187/314\n",
      "filtering for word 188/314\n",
      "filtering for word 189/314\n",
      "filtering for word 190/314\n",
      "filtering for word 191/314\n",
      "filtering for word 192/314\n",
      "filtering for word 193/314\n",
      "filtering for word 194/314\n",
      "filtering for word 195/314\n",
      "filtering for word 196/314\n",
      "filtering for word 197/314\n",
      "filtering for word 198/314\n",
      "filtering for word 199/314\n",
      "filtering for word 200/314\n",
      "filtering for word 201/314\n",
      "filtering for word 202/314\n",
      "filtering for word 203/314\n",
      "filtering for word 204/314\n",
      "filtering for word 205/314\n",
      "filtering for word 206/314\n",
      "filtering for word 207/314\n",
      "filtering for word 208/314\n",
      "filtering for word 209/314\n",
      "filtering for word 210/314\n",
      "filtering for word 211/314\n",
      "filtering for word 212/314\n",
      "filtering for word 213/314\n",
      "filtering for word 214/314\n",
      "filtering for word 215/314\n",
      "filtering for word 216/314\n",
      "filtering for word 217/314\n",
      "filtering for word 218/314\n",
      "filtering for word 219/314\n",
      "filtering for word 220/314\n",
      "filtering for word 221/314\n",
      "filtering for word 222/314\n",
      "filtering for word 223/314\n",
      "filtering for word 224/314\n",
      "filtering for word 225/314\n",
      "filtering for word 226/314\n",
      "filtering for word 227/314\n",
      "filtering for word 228/314\n",
      "filtering for word 229/314\n",
      "filtering for word 230/314\n",
      "filtering for word 231/314\n",
      "filtering for word 232/314\n",
      "filtering for word 233/314\n",
      "filtering for word 234/314\n",
      "filtering for word 235/314\n",
      "filtering for word 236/314\n",
      "filtering for word 237/314\n",
      "filtering for word 238/314\n",
      "filtering for word 239/314\n",
      "filtering for word 240/314\n",
      "filtering for word 241/314\n",
      "filtering for word 242/314\n",
      "filtering for word 243/314\n",
      "filtering for word 244/314\n",
      "filtering for word 245/314\n",
      "filtering for word 246/314\n",
      "filtering for word 247/314\n",
      "filtering for word 248/314\n",
      "filtering for word 249/314\n",
      "filtering for word 250/314\n",
      "filtering for word 251/314\n",
      "filtering for word 252/314\n",
      "filtering for word 253/314\n",
      "filtering for word 254/314\n",
      "filtering for word 255/314\n",
      "filtering for word 256/314\n",
      "filtering for word 257/314\n",
      "filtering for word 258/314\n",
      "filtering for word 259/314\n",
      "filtering for word 260/314\n",
      "filtering for word 261/314\n",
      "filtering for word 262/314\n",
      "filtering for word 263/314\n",
      "filtering for word 264/314\n",
      "filtering for word 265/314\n",
      "filtering for word 266/314\n",
      "filtering for word 267/314\n",
      "filtering for word 268/314\n",
      "filtering for word 269/314\n",
      "filtering for word 270/314\n",
      "filtering for word 271/314\n",
      "filtering for word 272/314\n",
      "filtering for word 273/314\n",
      "filtering for word 274/314\n",
      "filtering for word 275/314\n",
      "filtering for word 276/314\n",
      "filtering for word 277/314\n",
      "filtering for word 278/314\n",
      "filtering for word 279/314\n",
      "filtering for word 280/314\n",
      "filtering for word 281/314\n",
      "filtering for word 282/314\n",
      "filtering for word 283/314\n",
      "filtering for word 284/314\n",
      "filtering for word 285/314\n",
      "filtering for word 286/314\n",
      "filtering for word 287/314\n",
      "filtering for word 288/314\n",
      "filtering for word 289/314\n",
      "filtering for word 290/314\n",
      "filtering for word 291/314\n",
      "filtering for word 292/314\n",
      "filtering for word 293/314\n",
      "filtering for word 294/314\n",
      "filtering for word 295/314\n",
      "filtering for word 296/314\n",
      "filtering for word 297/314\n",
      "filtering for word 298/314\n",
      "filtering for word 299/314\n",
      "filtering for word 300/314\n",
      "filtering for word 301/314\n",
      "filtering for word 302/314\n",
      "filtering for word 303/314\n",
      "filtering for word 304/314\n",
      "filtering for word 305/314\n",
      "filtering for word 306/314\n",
      "filtering for word 307/314\n",
      "filtering for word 308/314\n",
      "filtering for word 309/314\n",
      "filtering for word 310/314\n",
      "filtering for word 311/314\n",
      "filtering for word 312/314\n",
      "filtering for word 313/314\n",
      "               image_id                                            caption  \\\n",
      "24188  7f2161e5ef85b2a8  In this image there are doctors in a room, beh...   \n",
      "16456  155115a50a8e5de1  On the background of the picture we can see st...   \n",
      "21753  73627fab0e12a5d3  In the image there is a person in doctors apro...   \n",
      "664    8669df2da990014a  In this image there are doctors and patient. T...   \n",
      "24076  44d660ce91486bd3  In this picture we can see one person is layin...   \n",
      "...                 ...                                                ...   \n",
      "81577            490051       FOUR SKI COMPETITORS WALKING UP A SKI SLOPE.   \n",
      "83216            522639  Snowboarding competitors and their supporters ...   \n",
      "38955            427384     A couple of sexy babes kissing a nerds cheeks.   \n",
      "73848            491064             Nerds gather in the nasa meeting room    \n",
      "26327             36907  Many people are milling across a tarmac area t...   \n",
      "\n",
      "           dataset  \n",
      "24188  open_images  \n",
      "16456  open_images  \n",
      "21753  open_images  \n",
      "664    open_images  \n",
      "24076  open_images  \n",
      "...            ...  \n",
      "81577         coco  \n",
      "83216         coco  \n",
      "38955         coco  \n",
      "73848         coco  \n",
      "26327         coco  \n",
      "\n",
      "[21614 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "result_dirs = ['open-images/result_csv', 'coco/result_csv']\n",
    "final_dir = '../results'\n",
    "captions = pd.DataFrame(columns=['image_id', 'caption', 'dataset'])\n",
    "for dir in result_dirs:\n",
    "    for csv in os.listdir(os.path.join(dataset_dir, dir)):\n",
    "        if 'captions' not in csv:\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(os.path.join(dataset_dir, dir), csv))\n",
    "        captions = pd.concat([captions, df])\n",
    "filtered = pd.DataFrame(columns=['image_id', 'caption', 'dataset'])\n",
    "# check if caption contains single\n",
    "for i, keyword in enumerate(person_plural_words):\n",
    "    print(f'filtering for word {i}/{len(person_plural_words)}')\n",
    "    filtered = pd.concat([filtered, captions[(captions['caption'].str.contains(f\"\\\\b{keyword}\\\\b\", case=False))]])\n",
    "    \n",
    "filtered = filtered[~filtered['caption'].str.contains('two')]\n",
    "filtered = filtered[~filtered['caption'].str.contains('Two')]\n",
    "filtered = filtered[~filtered['caption'].str.contains('TWO')]\n",
    "filtered = filtered[~filtered['caption'].str.contains('three')]\n",
    "filtered = filtered[~filtered['caption'].str.contains('Three')]\n",
    "filtered = filtered[~filtered['caption'].str.contains('THREE')]\n",
    "\n",
    "\n",
    "for word in ['women', 'Women', 'WOMEN', 'WOMAN', 'Woman', 'woman', 'man', 'MAN', 'Man', 'men', 'Men', 'MEN']:\n",
    "    filtered['caption'] = filtered['caption'].apply(lambda x: re.sub(f\"\\\\b{word}\\\\b\", 'person' if 'a' in word else 'persons', x))\n",
    "\n",
    "\n",
    "print(filtered)\n",
    "filtered.to_csv(os.path.join(final_dir, \"filtered_captions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b616defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_path = '../results/filtered_captions.csv'\n",
    "sample_dir = '../results/batches'\n",
    "df = pd.read_csv(final_csv_path)\n",
    "num_sample = 10\n",
    "total = 8000\n",
    "num_needed_samples = total / num_sample\n",
    "for i in range(int(num_needed_samples)):\n",
    "    sample = df.sample(n=num_sample)\n",
    "\n",
    "    # print(sample)\n",
    "    sample.to_csv(os.path.join(sample_dir, f'{i}.csv'), index=False)\n",
    "    df = df.drop(sample.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
